<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Memory Overload | Diary Of An Observer</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="An Issue was reported that videos were not being uploaded from google drive to S3. We have an asynchronous task that transfers files from google drive to S3 (for further processing).
My first assumption was that the tasks might have been queued. Also there weren&rsquo;t any sentry alerts, so I ignored the issue. Even after few hours, the issue was not resolved so it seemed something was cooking.
I tried to upload a 2MB file from drive and it got processed instantaneously. So, the pipeline was working fine. I looked at the file that they were trying to upload. It was a whopping 8.3GB file. That raised some doubts. Is google drive blocking such file ? Is some network connection getting reset ? Is S3 limiting file upload ?">
    <meta name="generator" content="Hugo 0.145.0">
    
    
    
      <meta name="robots" content="index, follow">
    
    

    
<link rel="stylesheet" href="/ananke/css/main.min.d05fb5f317fcf33b3a52936399bdf6f47dc776516e1692e412ec7d76f4a5faa2.css" >




    


    
      

    

    

    
      <link rel="canonical" href="http://pvsukalkar.in/tech/memory-overload/">
    

    <meta property="og:url" content="http://pvsukalkar.in/tech/memory-overload/">
  <meta property="og:site_name" content="Diary Of An Observer">
  <meta property="og:title" content="Memory Overload">
  <meta property="og:description" content="An Issue was reported that videos were not being uploaded from google drive to S3. We have an asynchronous task that transfers files from google drive to S3 (for further processing).
My first assumption was that the tasks might have been queued. Also there weren’t any sentry alerts, so I ignored the issue. Even after few hours, the issue was not resolved so it seemed something was cooking.
I tried to upload a 2MB file from drive and it got processed instantaneously. So, the pipeline was working fine. I looked at the file that they were trying to upload. It was a whopping 8.3GB file. That raised some doubts. Is google drive blocking such file ? Is some network connection getting reset ? Is S3 limiting file upload ?">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="tech">
    <meta property="article:published_time" content="2025-04-18T21:02:55+05:30">
    <meta property="article:modified_time" content="2025-04-18T21:02:55+05:30">

  <meta itemprop="name" content="Memory Overload">
  <meta itemprop="description" content="An Issue was reported that videos were not being uploaded from google drive to S3. We have an asynchronous task that transfers files from google drive to S3 (for further processing).
My first assumption was that the tasks might have been queued. Also there weren’t any sentry alerts, so I ignored the issue. Even after few hours, the issue was not resolved so it seemed something was cooking.
I tried to upload a 2MB file from drive and it got processed instantaneously. So, the pipeline was working fine. I looked at the file that they were trying to upload. It was a whopping 8.3GB file. That raised some doubts. Is google drive blocking such file ? Is some network connection getting reset ? Is S3 limiting file upload ?">
  <meta itemprop="datePublished" content="2025-04-18T21:02:55+05:30">
  <meta itemprop="dateModified" content="2025-04-18T21:02:55+05:30">
  <meta itemprop="wordCount" content="302">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Memory Overload">
  <meta name="twitter:description" content="An Issue was reported that videos were not being uploaded from google drive to S3. We have an asynchronous task that transfers files from google drive to S3 (for further processing).
My first assumption was that the tasks might have been queued. Also there weren’t any sentry alerts, so I ignored the issue. Even after few hours, the issue was not resolved so it seemed something was cooking.
I tried to upload a 2MB file from drive and it got processed instantaneously. So, the pipeline was working fine. I looked at the file that they were trying to upload. It was a whopping 8.3GB file. That raised some doubts. Is google drive blocking such file ? Is some network connection getting reset ? Is S3 limiting file upload ?">

      
    
	
  </head><body class="ma0 avenir bg-near-white production">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l center items-center justify-between">
    <a href="/" class="f3 fw2 hover-white white-90 dib no-underline">
      
        Diary Of An Observer
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/" title="Home page">
              Home
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/tech/" title="Tech page">
              Tech
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/finance/" title="Finance page">
              Finance
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/about/" title="About Me page">
              About Me
            </a>
          </li>
          
        </ul>
      
      <div class="ananke-socials"></div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  
  <article class="flex-l mw8 center ph3 flex-wrap justify-between">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Tech
      </aside><div id="sharing" class="mt3 ananke-socials"></div>
<h1 class="f1 athelas mt3 mb1">Memory Overload</h1>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2025-04-18T21:02:55+05:30">April 18, 2025</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>An Issue was reported that videos were not being uploaded from google drive to S3. We have an asynchronous task that transfers files from google drive to S3 (for further processing).</p>
<p>My first assumption was that the tasks might have been queued. Also there weren&rsquo;t any sentry alerts, so I ignored the issue. Even after few hours, the issue was not resolved so it seemed something was cooking.</p>
<p>I tried to upload a 2MB file from drive and it got processed instantaneously. So, the pipeline was working fine. I looked at the file that they were trying to upload. It was a whopping 8.3GB file. That raised some doubts. Is google drive blocking such file ? Is some network connection getting reset ? Is S3 limiting file upload ?</p>
<p>I added few loggers and ran the task synchronously for mentioned file. It was progressing fine. Then suddenly it stopped at 65% . Waited for about a minute and then task got killed. 65% of 8.3GB was somewhere around 5GB. I tried to check the file locally where it was being stored. But there wasn&rsquo;t any file to be found.</p>
<p>We were storing the file in memory rather than writing it on disk.</p>
<pre tabindex="0"><code>fh = io.BytesIO()
</code></pre><p>The machine on which task was running had 8GB of RAM and 3GB was already utilized. So, it could only store 5GB of downloaded file and the process got killed when memory was 100% utilized.</p>
<p>Sentry alert was not raised as the process was killed by linux due to memory overload.</p>
<p>To fix the issue, I stored the file locally in temp directory with <code>io.FileIO()</code> instead of <code>io.BytesIO()</code>. Uploaded the file to S3 with <code>multi-part</code> upload (as recommended for larger files). Finally cleared the temp file after upload.</p>
<p>Eventually, we could upload 10+GB files without any issue.</p>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href="http://pvsukalkar.in/" >
    &copy;  Diary Of An Observer 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>
