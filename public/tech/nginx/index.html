<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Nginx | Diary Of An Observer</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Nginx can be used for multiple purposes like web serving, load balancing or reverse proxying. It was designed to solve C10k problem so obviously it can handle large number of connections simultaneously. Besides, it also excels at serving static content. But how does it achieve this ?
Nginx follows an asynchronous event-driven architecture. There is a master process that manages all worker processes. Typically there are as many workers as there are CPU cores. Each worker has an event loop running inside it. When an event comes on socket, OS notifies worker. Worker registers interest in the event and moves ahead. When data is received from client, a read event is triggered which is then processed by worker. Thus, the worker continues with its events without being blocked.">
    <meta name="generator" content="Hugo 0.145.0">
    
    
    
      <meta name="robots" content="index, follow">
    
    

    
<link rel="stylesheet" href="/ananke/css/main.min.d05fb5f317fcf33b3a52936399bdf6f47dc776516e1692e412ec7d76f4a5faa2.css" >




    


    
      

    

    

    
      <link rel="canonical" href="http://pvsukalkar.in/tech/nginx/">
    

    <meta property="og:url" content="http://pvsukalkar.in/tech/nginx/">
  <meta property="og:site_name" content="Diary Of An Observer">
  <meta property="og:title" content="Nginx">
  <meta property="og:description" content="Nginx can be used for multiple purposes like web serving, load balancing or reverse proxying. It was designed to solve C10k problem so obviously it can handle large number of connections simultaneously. Besides, it also excels at serving static content. But how does it achieve this ?
Nginx follows an asynchronous event-driven architecture. There is a master process that manages all worker processes. Typically there are as many workers as there are CPU cores. Each worker has an event loop running inside it. When an event comes on socket, OS notifies worker. Worker registers interest in the event and moves ahead. When data is received from client, a read event is triggered which is then processed by worker. Thus, the worker continues with its events without being blocked.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="tech">
    <meta property="article:published_time" content="2025-04-20T21:56:59+05:30">
    <meta property="article:modified_time" content="2025-04-20T21:56:59+05:30">

  <meta itemprop="name" content="Nginx">
  <meta itemprop="description" content="Nginx can be used for multiple purposes like web serving, load balancing or reverse proxying. It was designed to solve C10k problem so obviously it can handle large number of connections simultaneously. Besides, it also excels at serving static content. But how does it achieve this ?
Nginx follows an asynchronous event-driven architecture. There is a master process that manages all worker processes. Typically there are as many workers as there are CPU cores. Each worker has an event loop running inside it. When an event comes on socket, OS notifies worker. Worker registers interest in the event and moves ahead. When data is received from client, a read event is triggered which is then processed by worker. Thus, the worker continues with its events without being blocked.">
  <meta itemprop="datePublished" content="2025-04-20T21:56:59+05:30">
  <meta itemprop="dateModified" content="2025-04-20T21:56:59+05:30">
  <meta itemprop="wordCount" content="548">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Nginx">
  <meta name="twitter:description" content="Nginx can be used for multiple purposes like web serving, load balancing or reverse proxying. It was designed to solve C10k problem so obviously it can handle large number of connections simultaneously. Besides, it also excels at serving static content. But how does it achieve this ?
Nginx follows an asynchronous event-driven architecture. There is a master process that manages all worker processes. Typically there are as many workers as there are CPU cores. Each worker has an event loop running inside it. When an event comes on socket, OS notifies worker. Worker registers interest in the event and moves ahead. When data is received from client, a read event is triggered which is then processed by worker. Thus, the worker continues with its events without being blocked.">

      
    
	
  </head><body class="ma0 avenir bg-near-white production">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l center items-center justify-between">
    <a href="/" class="f3 fw2 hover-white white-90 dib no-underline">
      
        Diary Of An Observer
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/" title="Home page">
              Home
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/tech/" title="Tech page">
              Tech
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/finance/" title="Finance page">
              Finance
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/about/" title="About Me page">
              About Me
            </a>
          </li>
          
        </ul>
      
      <div class="ananke-socials"></div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  
  <article class="flex-l mw8 center ph3 flex-wrap justify-between">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Tech
      </aside><div id="sharing" class="mt3 ananke-socials"></div>
<h1 class="f1 athelas mt3 mb1">Nginx</h1>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2025-04-20T21:56:59+05:30">April 20, 2025</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>Nginx can be used for multiple purposes like web serving, load balancing or reverse proxying. It was designed to solve C10k problem so obviously it can handle large number of connections simultaneously. Besides, it also excels at serving static content. But how does it achieve this ?</p>
<p>Nginx follows an asynchronous event-driven architecture. There is a master process that manages all worker processes. Typically there are as many workers as there are CPU cores. Each worker has an event loop running inside it. When an event comes on socket, OS notifies worker. Worker registers interest in the event and moves ahead. When data is received from client, a read event is triggered which is then processed by worker. Thus, the worker continues with its events without being blocked.</p>
<p>Similarly when an event is received for static file, nginx directly transfers that file using <code>sendfile()</code> (just like kafka). This bypasses the user space. File is directly transferred from kernel space without ever being brought into user space thus improving performance. Also, for larger files nginx implements Asynchronous IO so that its workers aren&rsquo;t blocked.</p>
<h2 id="thus-with-asynchronous-event-driven-architecture-and-efficiently-leveraging-os-capabilities-nginx-is-able-to-deliver-terrific-performance-with-little-memory-footprint">Thus, with asynchronous event-driven architecture and efficiently leveraging OS capabilities Nginx is able to deliver terrific performance with little memory footprint.</h2>
<p>Too much of tech-talk. How do I use nginx ?</p>
<p>Nginx loads its config from <code>/etc/nginx/nginx.conf</code> file. With <code>include</code> directive we can import multiple config files.</p>
<p>For most practical purposes there are few important blocks to note:</p>
<ul>
<li><code>http</code> : operates at application (i.e. level 7) and can be used to filter requests</li>
<li><code>stream</code> : operates at level 4 and can be used to filter TCP / UDP connections or IP addresses</li>
<li><code>events</code> : for event level settings of workers</li>
<li><code>server</code> : single server level block</li>
</ul>
<p>There are few important directives to note:</p>
<ul>
<li><code>listen</code> : to tell which port to listen on</li>
<li><code>location</code> : the path that should be matched when request comes in. Request is served with the first matched location logic. So, if there are any regex patterns, they should be kept at the end</li>
<li><code>upstream</code> : to group servers (used for load balancing)</li>
<li><code>proxy_*</code> : when nginx acts as proxy / load balancer and forwards request to some upstream</li>
</ul>
<hr>
<p>With this knowledge, let&rsquo;s try to configure a simple load balancer. Say, there are 3 servers <code>server1</code>, <code>server2</code>, <code>server3</code> . For testing assume that they will be serving <code>index.html</code> file.</p>
<p>Config of each server can look like :</p>
<pre tabindex="0"><code>server {
    listen 8081;
    root /usr/share/nginx/html/server1;
    index index.html;

    location / {
        try_files $uri $uri/ =404;
    }
}
</code></pre><p>It means <code>server1</code> is listening on port 8081 and serving <code>index.html</code> file.</p>
<p>We can create similar config for <code>server2</code> and <code>server3</code> .</p>
<p>Now let&rsquo;s group these servers with <code>upstream</code> directive.</p>
<pre tabindex="0"><code>upstream backend {
    server 127.0.0.1:8081;
    server 127.0.0.1:8082;
    server 127.0.0.1:8083;
}
</code></pre><p>Finally, lets use these <code>backend</code> servers to serve requests. We haven&rsquo;t provided any algo so servers will be picked in round-robin fashion.</p>
<pre tabindex="0"><code>server {
    listen 80;
    server_name _;
    
    location / {
        proxy_pass http://backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
</code></pre><p>We are telling nginx to listen on port 80 (i.e. http) traffic. When a request comes, we are asking it to direct locally to one of the <code>backend</code> servers (in round-robin fashion). Thus, requests will be distributed among servers and we have configured a simple load balancer.</p>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href="http://pvsukalkar.in/" >
    &copy;  Diary Of An Observer 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>
